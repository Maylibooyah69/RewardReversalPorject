{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%config IPCompleter.greedy=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid function for nd; return the value of sigmoid with argument beta\n",
    "# q can be n dimensional, either list or int/float\n",
    "# beta should be between 0 and 1; determine the sharpness of sigmoid\n",
    "# we can also try different functions here\n",
    "def sigmoid(beta,q):\n",
    "    q=np.array(q)\n",
    "    val = 1/(1+np.exp(0-beta*q))\n",
    "    return val\n",
    "def ratchose(Q): # 0 means left\n",
    "    if random.uniform(0,1)<sigmoid(beta,Q[0]-Q[1]):\n",
    "        return np.array([1,0])\n",
    "    else:\n",
    "        return np.array([0,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class toyQ_2choice:\n",
    "    '''a unchanging env with probability of giving one\n",
    "    of two rewards based on params to two different choices'''\n",
    "    obs_size=1\n",
    "    action_size=2\n",
    "    def __init__(self,left=0.2,right=0.8,state=0):\n",
    "        self.left=left\n",
    "        self.right=right\n",
    "        self.obslog=[]\n",
    "        self.state=state\n",
    "    def step(self,action,state=0):\n",
    "        '''takes in the action param as a int of 0 or 1\n",
    "        where 0 is go left and 1 is to right, and returns\n",
    "        a return reward where 0 is no reward'''\n",
    "        reward_site = random.uniform(0,1)\n",
    "        if reward_site < self.right and np.array_equal(action,np.array([0,1])):\n",
    "            #if the rat goes right and the reward is on the right\n",
    "            obs=np.array([0,1])\n",
    "        elif reward_site >= self.right and np.array_equal(action,np.array([1,0])):\n",
    "            obs=np.array([1,0])\n",
    "        else:\n",
    "            obs=np.array([0,0])\n",
    "        self.obslog.append(obs)\n",
    "        return obs\n",
    "    \n",
    "class WSLS_rat: # to-do\n",
    "    pass\n",
    "\n",
    "class sig_rat:\n",
    "    def __init__(self,env,alpha=0.2,beta=-4,gamma=0):\n",
    "        # alpha-learning rate, beta-sigmoid slop, gamma-discount factor of future reward\n",
    "        '''Takes in the beta, gamma ,and the \n",
    "        environment of the rat'''\n",
    "        self.choiceLog=[]\n",
    "        self.beta=beta\n",
    "        self.gamma=gamma\n",
    "        self.env=env\n",
    "        self.alpha=alpha\n",
    "        self.action_size=env.action_size\n",
    "        self.obs_size=env.obs_size\n",
    "        self.Q=np.zeros((self.obs_size,)+(self.action_size,)) # Q-table starts from 0\n",
    "        self.Qlog=np.zeros((self.obs_size,)+(self.action_size,))\n",
    "    def get_choice(self): # 0 means left\n",
    "        if random.uniform(0,1)<sigmoid(self.beta,self.Q[self.env.state][0]-self.Q[self.env.state][1]):\n",
    "            self.choice=np.array([1,0])\n",
    "        else:\n",
    "            self.choice=np.array([0,1])\n",
    "        self.choiceLog.append(self.choice)\n",
    "        return self.choice\n",
    "    def update(self,obs):\n",
    "        '''Takes obs/reward and update its Q-table'''\n",
    "        self.Q[self.env.state] = self.Q[self.env.state] + self.alpha*self.choice*\\\n",
    "        (obs + self.gamma*np.max(self.Q[self.env.state])-self.Q[self.env.state])\n",
    "        self.Qlog=np.vstack((self.Qlog,self.Q))\n",
    "        return self.Q[self.env.state]\n",
    "    \n",
    "class FSrat:\n",
    "    def __init__(self,env,alphaF=0.2,alphaS=0.1,beta=-4,gamma=0): # fail and success\n",
    "        # alpha-learning rate, beta-sigmoid slop, gamma-discount factor of future reward\n",
    "        '''Takes in the beta, gamma ,and the \n",
    "        environment of the rat'''\n",
    "        self.choiceLog=[]\n",
    "        self.beta=beta\n",
    "        self.env=env\n",
    "        self.alphaF=alphaF\n",
    "        self.gamma=gamma\n",
    "        self.alphaS=alphaS\n",
    "        self.action_size=env.action_size\n",
    "        self.obs_size=env.obs_size\n",
    "        self.Q=np.zeros((self.obs_size,)+(self.action_size,)) # Q-table starts from 0\n",
    "        self.Qlog=np.zeros((self.obs_size,)+(self.action_size,))\n",
    "    def get_choice(self): #[1,0] means left\n",
    "        if random.uniform(0,1)<sigmoid(beta,self.Q[self.env.state][0]-self.Q[self.env.state][1]):\n",
    "            self.choice=np.array([1,0])\n",
    "        else:\n",
    "            self.choice=np.array([0,1])\n",
    "        self.choiceLog.append(self.choice)\n",
    "        return self.choice\n",
    "    def update(self,obs):\n",
    "        '''Takes obs/reward and update its Q-table'''\n",
    "        used_alpha=[self.alphaF,self.alphaS][max(obs)]\n",
    "        self.Q[self.env.state] = self.Q[self.env.state] + used_alpha*self.choice*\\\n",
    "        (obs + self.gamma*np.max(self.Q[self.env.state])-self.Q[self.env.state])\n",
    "        self.Qlog=np.vstack((self.Qlog,self.Q))\n",
    "        return self.Q[self.env.state]\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_rat(env,rat,it_num,every=500):\n",
    "    for i in range(it_num):\n",
    "        action=rat.get_choice()\n",
    "        obs=env.step(action)\n",
    "        rat.update(obs)\n",
    "        if i%every==-1:\n",
    "            print(np.mean(rat.Qlog,axis=0))\n",
    "    return env,rat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_log: [865 416]\n",
      "choice_log: [4481  519]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.21009495, 0.80711444]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=toyQ_2choice()\n",
    "rat=sig_rat(env,alpha=0.01)\n",
    "env,rat=train_rat(env,rat,5000)\n",
    "obslog=np.array(env.obslog)\n",
    "choicelog=np.array(rat.choiceLog) \n",
    "print('reward_log:',np.sum(obslog,axis=0))\n",
    "print('choice_log:',np.sum(choicelog,axis=0))\n",
    "rat.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
