{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maylibooyah69/anaconda2/envs/py3/lib/python3.7/site-packages/pandas/core/indexes/base.py:3969: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ToyQ2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('Data/UH_RL_rats.csv');\n",
    "df = df.drop(df.columns[[range(24)]],axis=1);\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging into respective subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = df['subject'].unique();\n",
    "sub_df={} # a dictionary whose keys is the subject number\n",
    "for i in subjects:\n",
    "    sub_df[i]=df[df['subject']==i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/UH_RL_rats.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on the Data:\n",
    "# the update rule used for the given data is different than ours\n",
    "### There are 24 subjects (1 to 24)\n",
    "### each subject has 5 sessions (['1', '11', '16', '6', 'Best'])\n",
    "### each the rat chose between ([1., 2.]) which turns out to be (['lean', 'rich']) depending on the experiment and then get a reward ([0., 1.])\n",
    "### reward is the same as response, action is the same as lever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Logs(sub_df):\n",
    "    '''Returns a dict of DataFrame indexs as dict[rat number][session name]\n",
    "    where rat is int from 1 to 24 and session name is 1, 11, 16,6, Best as strings'''\n",
    "    Logs={}\n",
    "    for i in sub_df:\n",
    "        rat=sub_df[i]\n",
    "        sessions=rat['session'].unique()\n",
    "        Logs[i]={}\n",
    "        for j,sesh in enumerate(sessions):\n",
    "            Logs[i][sesh]={}\n",
    "            Logs[i][sesh]=rat[rat['session']==sesh]\n",
    "    return Logs\n",
    "epoched_df=get_Logs(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>subject</th>\n",
       "      <th>lever</th>\n",
       "      <th>response</th>\n",
       "      <th>feedback</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>alpha_gain</th>\n",
       "      <th>alpha_loss</th>\n",
       "      <th>beta</th>\n",
       "      <th>Q</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14113</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>lean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.742804</td>\n",
       "      <td>0.453742</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>rich</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.742804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14115</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>lean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.742804</td>\n",
       "      <td>0.411764</td>\n",
       "      <td>-0.453742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14116</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>lean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.742804</td>\n",
       "      <td>0.373670</td>\n",
       "      <td>-0.411764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14117</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>rich</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.742804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session  subject lever response  feedback  state  action  reward  \\\n",
       "14113       1        1     A     lean       0.0    1.0     1.0     0.0   \n",
       "14114       1        1     B     rich       1.0    1.0     2.0     1.0   \n",
       "14115       1        1     A     lean       0.0    1.0     1.0     0.0   \n",
       "14116       1        1     A     lean       0.0    1.0     1.0     0.0   \n",
       "14117       1        1     B     rich       1.0    1.0     2.0     1.0   \n",
       "\n",
       "       alpha_gain  alpha_loss      beta         Q        PE  \n",
       "14113         1.0    0.092515  0.742804  0.453742 -0.500000  \n",
       "14114         1.0    0.092515  0.742804  1.000000  0.500000  \n",
       "14115         1.0    0.092515  0.742804  0.411764 -0.453742  \n",
       "14116         1.0    0.092515  0.742804  0.373670 -0.411764  \n",
       "14117         1.0    0.092515  0.742804  1.000000  0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoched_df[1]['1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_env():\n",
    "    def __init__(self,epoched_df):\n",
    "        '''takes in the data of a rat over one single trial'''\n",
    "        self.epoched_df=epoched_df\n",
    "        self.Q=epoched_df['Q']\n",
    "        self.reward=epoched_df['reward']\n",
    "        self.test_alphaL=epoched_df['alpha_loss'].iloc[0] # stored as scaler\n",
    "        self.test_alphaG=epoched_df['alpha_gain'].iloc[0]\n",
    "        self.test_beta=epoched_df['beta'].iloc[0]\n",
    "        self.PE=epoched_df['PE']\n",
    "        self.count=0 # counting from 0\n",
    "    def step(self):\n",
    "        temp=self.reward.iloc[self.count]+0\n",
    "        self.count+=1\n",
    "        return temp\n",
    "    def get_switchid(self):\n",
    "        pass\n",
    "    def init_Q(self):\n",
    "        left=self.epoched_df[self.epoched_df['action']==1]['Q'].iloc[0]\n",
    "        right=self.epoched_df[self.epoched_df['action']==2]['Q'].iloc[0]\n",
    "        return np.array([right,left])\n",
    "\n",
    "class Rat():\n",
    "    def __init__(self,epoched_df,alphaG=None,alphaL=None,beta=None,gamma=0,init_Q=np.array([-1,-1])):\n",
    "        self.gamma=gamma\n",
    "        self.df=epoched_df\n",
    "        self.actions=epoched_df['action']\n",
    "        self.count=0\n",
    "        self.PE=0 # prediction error (Q-R)\n",
    "        self.alphaG=epoched_df['alpha_gain'].iloc[1]\n",
    "        self.alphaL=epoched_df['alpha_loss'].iloc[1]\n",
    "        self.beta=epoched_df['beta'].iloc[1]\n",
    "        if (init_Q==np.array([-1,-1])).all():\n",
    "            self.Q=np.random.rand(2) # Q[0] represent left\n",
    "        else:\n",
    "            self.Q=init_Q\n",
    "    def get_action(self):\n",
    "        temp=self.count+0\n",
    "        self.count+=1\n",
    "        return self.actions.iloc[temp]\n",
    "    def update(self,obs): # 1 represent left \n",
    "        action_id=int(self.get_action()-1)\n",
    "        if int(obs)==1: # alpha_gain\n",
    "            self.Q[action_id]=(1-self.alphaG)*self.Q[action_id]+self.alphaG*(obs+self.gamma*np.max(self.Q))\n",
    "        elif int(obs)==0: # alpha_loss\n",
    "            self.Q[action_id]=(1-self.alphaL)*self.Q[action_id]+self.alphaL*(obs+self.gamma*np.max(self.Q))\n",
    "        else:\n",
    "            print('error')\n",
    "        return self.Q[action_id]\n",
    "    \n",
    "    \n",
    "def train_rat(env,rat,it_num):\n",
    "    QLog=rat.Q\n",
    "    qlog=[0]\n",
    "    for i in range(it_num):\n",
    "        obs=env.step()\n",
    "        q=rat.update(obs)\n",
    "        QLog=np.vstack((QLog,rat.Q))\n",
    "        qlog.append(q)\n",
    "    return QLog,qlog\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "## the estimation is still off because the initialization value is different and i just put it as the first updated value, but overall it would converge to the same value\n",
    "## the data is processed into a Dataframe <epoched_df> whose first index is the subject number and second is the session number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.3869416399847319,\n",
       " 0.6633951687563733,\n",
       " 0.8151842959718735,\n",
       " 0.8985253885714726,\n",
       " 0.944284514031321,\n",
       " 0.9694089454196876,\n",
       " 0.5170498712651,\n",
       " 0.7348321837127378,\n",
       " 0.39193457805945187,\n",
       " 0.20904461846310635,\n",
       " 0.5973661544605066,\n",
       " 0.11149731346683318,\n",
       " 0.7789305122151531,\n",
       " 0.059468887559596686,\n",
       " 0.03171868879717861,\n",
       " 0.8786199447193247,\n",
       " 0.016917673430565296,\n",
       " 0.46023040836559964,\n",
       " 0.2454712993128988,\n",
       " 0.9333552632361521,\n",
       " 0.49782005930626844,\n",
       " 0.7242738942717653,\n",
       " 0.8486102705037684,\n",
       " 0.5857196924010148,\n",
       " 0.3124030296855442,\n",
       " 0.16662518645503865,\n",
       " 0.9168782000659313,\n",
       " 0.9543612790162802,\n",
       " 0.08887222633251281,\n",
       " 0.047401433008620306,\n",
       " 0.5090239560788589,\n",
       " 0.025282317592270325,\n",
       " 0.7304254877062222,\n",
       " 0.8519878544418346,\n",
       " 0.9187326908381568,\n",
       " 0.9553795027191789,\n",
       " 0.013484731204649006,\n",
       " 0.9755007419573506,\n",
       " 0.5202990290436145,\n",
       " 0.007192298530306761,\n",
       " 0.7366161610255888,\n",
       " 0.8553868955182795,\n",
       " 0.4562343748727263,\n",
       " 0.45489060975817014,\n",
       " 0.7014409256608249,\n",
       " 0.8360736377011667,\n",
       " 0.24262323173976774,\n",
       " 0.9099948567431555,\n",
       " 0.4853604103346209,\n",
       " 0.7174328195666559,\n",
       " 0.8448541210947936,\n",
       " 0.9148158547487542,\n",
       " 0.9532289310332314,\n",
       " 0.9743199525470119,\n",
       " 0.9859001546948546,\n",
       " 0.5841559370273823,\n",
       " 0.31156897557613344,\n",
       " 0.6220111755864458,\n",
       " 0.7924620676398042,\n",
       " 0.8860495586472141,\n",
       " 0.9374345550385512,\n",
       " 0.49999581527721226,\n",
       " 0.7254685113248769,\n",
       " 0.3869403126097803,\n",
       " 0.5258457232871169,\n",
       " 0.7396616199925745,\n",
       " 0.8570590302912496,\n",
       " 0.9215170624450566,\n",
       " 0.9569082852886504,\n",
       " 0.9763401328414295,\n",
       " 0.9870093516187286,\n",
       " 0.9928673756181805,\n",
       " 0.9960837727972477,\n",
       " 0.9978497626283714,\n",
       " 0.5322192391009393,\n",
       " 0.2063808465637265,\n",
       " 0.5642567516614997,\n",
       " 0.7607515170841521,\n",
       " 0.2838677014093514,\n",
       " 0.1514054096194481,\n",
       " 0.4057590718876628,\n",
       " 0.6737270374710691,\n",
       " 0.8208570950917727,\n",
       " 0.9016400864778598,\n",
       " 0.9459946650243328,\n",
       " 0.970347918156948,\n",
       " 0.9837192759192548,\n",
       " 0.9910609319778515,\n",
       " 0.5285983069541121,\n",
       " 0.2819364189415936,\n",
       " 0.534072027201152,\n",
       " 0.15037532901616008,\n",
       " 0.7441783411097511,\n",
       " 0.3969195016067139,\n",
       " 0.6688735973147178,\n",
       " 0.818192273092218,\n",
       " 0.4363960240631789,\n",
       " 0.6905485128630724,\n",
       " 0.533506452811744,\n",
       " 0.28455425756979014,\n",
       " 0.15177159540311386,\n",
       " 0.3683151691194479,\n",
       " 0.08094982436014682,\n",
       " 0.19644682636567562,\n",
       " 0.5588023946044449,\n",
       " 0.757756756623359,\n",
       " 0.8669943167324685,\n",
       " 0.9269721147435365,\n",
       " 0.9599034274775755,\n",
       " 0.0431758923435826,\n",
       " 0.9779846407656485,\n",
       " 0.9879122824788457,\n",
       " 0.9933631373753268,\n",
       " 0.9963559749455018,\n",
       " 0.5314225033860746,\n",
       " 0.023028557435417932,\n",
       " 0.7427235978506788,\n",
       " 0.012282651933079623,\n",
       " 0.006551150193939478,\n",
       " 0.8587402348998504,\n",
       " 0.003494161447982395,\n",
       " 0.4580229324301344,\n",
       " 0.7024229481794737,\n",
       " 0.8366128253963789,\n",
       " 0.9102909022667024,\n",
       " 0.9507444679446462,\n",
       " 0.9729558372633651,\n",
       " 0.985151175764305,\n",
       " 0.9918471285900123,\n",
       " 0.9955235976146846,\n",
       " 0.5309785415330542,\n",
       " 0.0018636672741622112,\n",
       " 0.451964880102205,\n",
       " 0.2410627467152373,\n",
       " 0.2832059553827804,\n",
       " 0.6064382216094291,\n",
       " 0.7839116067256624,\n",
       " 0.5832991396446223,\n",
       " 0.7712068998202671,\n",
       " 0.8743792305943154,\n",
       " 0.9310268635999974,\n",
       " 0.49657816986650644,\n",
       " 0.41811187864090227,\n",
       " 0.2230067032567917,\n",
       " 0.723592024465302,\n",
       " 0.8482358841710724,\n",
       " 0.45242026781918765,\n",
       " 0.699346758213097,\n",
       " 0.8349238175106047,\n",
       " 0.9093635383296932,\n",
       " 0.9502352910018335,\n",
       " 0.9726762693949743,\n",
       " 0.5187925511204752,\n",
       " 0.2767063611673238,\n",
       " 0.5733853176373565,\n",
       " 0.3058242922870316,\n",
       " 0.14758579348353248,\n",
       " 0.6188570090744223,\n",
       " 0.7907302460780405,\n",
       " 0.8850986875025371,\n",
       " 0.07871725950308975,\n",
       " 0.4720816375714059,\n",
       " 0.04198511792646332,\n",
       " 0.4739938988822946,\n",
       " 0.7101419980778615,\n",
       " 0.8408510344444429,\n",
       " 0.9126179264693627,\n",
       " 0.9520221400883175,\n",
       " 0.9736573538633417,\n",
       " 0.9855363493336595,\n",
       " 0.9920586113668052,\n",
       " 0.995639714005939,\n",
       " 0.5310404740801321,\n",
       " 0.28323898810405357,\n",
       " 0.711191941179248,\n",
       " 0.8414275145049199,\n",
       " 0.9129344476786443,\n",
       " 0.9521959287113657,\n",
       " 0.5078690316254691,\n",
       " 0.7297913667544702,\n",
       " 0.38924597716335135,\n",
       " 0.1510700752540053,\n",
       " 0.6646603843993315,\n",
       " 0.08057565729216147,\n",
       " 0.4951823579883214,\n",
       " 0.8158789732257228,\n",
       " 0.8989068069405084,\n",
       " 0.9444939349839025,\n",
       " 0.9695239297490765,\n",
       " 0.9832668581772125,\n",
       " 0.9908125282244014,\n",
       " 0.9949555416118884,\n",
       " 0.5306755597166548,\n",
       " 0.26411348446941046,\n",
       " 0.2830443551521397,\n",
       " 0.15096626463496107,\n",
       " 0.08052028822684969,\n",
       " 0.14086917991577294,\n",
       " 0.4951519571389065,\n",
       " 0.2640972696868354,\n",
       " 0.5282870219672925,\n",
       " 0.7410020354959784,\n",
       " 0.8577949966588045,\n",
       " 0.9219211509480593,\n",
       " 0.9571301534683083,\n",
       " 0.9764619514251778,\n",
       " 0.5208117055914708,\n",
       " 0.7368976503814626,\n",
       " 0.595946437558379,\n",
       " 0.7781510047499216,\n",
       " 0.3930362279670707,\n",
       " 0.4150393688151581,\n",
       " 0.6788224622868935,\n",
       " 0.6667414533731323,\n",
       " 0.8236547807969354,\n",
       " 0.3556172905795042,\n",
       " 0.9031761792645816,\n",
       " 0.9468380696444675,\n",
       " 0.9708109965330808,\n",
       " 0.9839735329832101,\n",
       " 0.9912005339500081,\n",
       " 0.5286727659139484,\n",
       " 0.7412138314432095,\n",
       " 0.857911284998913,\n",
       " 0.6461962721536086,\n",
       " 0.34465918734055184,\n",
       " 0.4575807986520318,\n",
       " 0.18382952755569565,\n",
       " 0.2440580873059579,\n",
       " 0.5518747609341725,\n",
       " 0.5849437565030101,\n",
       " 0.311989171779581,\n",
       " 0.16640444867661053,\n",
       " 0.08875449228388568,\n",
       " 0.753953081289021,\n",
       " 0.8649058769076067,\n",
       " 0.9258254393523183,\n",
       " 0.9592738357425528,\n",
       " 0.9776389581462737,\n",
       " 0.9877224825391537,\n",
       " 0.993258926127529,\n",
       " 0.9962987568863959,\n",
       " 0.5313919852128779,\n",
       " 0.2834264722270268,\n",
       " 0.0473386376579339,\n",
       " 0.47693329376425364,\n",
       " 0.15117007293001763,\n",
       " 0.5339428136192196,\n",
       " 0.2847869976331003,\n",
       " 0.15189573106365353,\n",
       " 0.7128058405773015,\n",
       " 0.38018647326175187,\n",
       " 0.08101603411363086,\n",
       " 0.6596861878450705,\n",
       " 0.8131478489137112,\n",
       " 0.897407260244616,\n",
       " 0.9436705963012685,\n",
       " 0.5033219083025824,\n",
       " 0.7272947305798878,\n",
       " 0.8502688859995018,\n",
       " 0.9177888768064386,\n",
       " 0.48951747644113747,\n",
       " 0.7197152915571167,\n",
       " 0.49542415057340355,\n",
       " 0.38387174020353704,\n",
       " 0.7229584004993925,\n",
       " 0.204744174058163,\n",
       " 0.10920360219414008,\n",
       " 0.847887987621531,\n",
       " 0.05824549971706771,\n",
       " 0.4522347115923714,\n",
       " 0.031066175191361775,\n",
       " 0.6992448770114246,\n",
       " 0.8348678787828545,\n",
       " 0.9093328247023414,\n",
       " 0.48500730488506466,\n",
       " 0.7172389440598346,\n",
       " 0.3825509403896638,\n",
       " 0.6609844183406991,\n",
       " 0.467998761850616,\n",
       " 0.8138606532491508,\n",
       " 0.2496146757365923,\n",
       " 0.5879946479563741,\n",
       " 0.313616413864213,\n",
       " 0.1672723644456484,\n",
       " 0.8977986314397408,\n",
       " 0.9438854819364392,\n",
       " 0.9691898535032913,\n",
       " 0.983083430814225,\n",
       " 0.08921740913520039,\n",
       " 0.5243433782481629,\n",
       " 0.04758554181485303,\n",
       " 0.2796669841998905,\n",
       " 0.6044951198176794]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf=epoched_df[12]['Best']\n",
    "rat=Rat(mydf,init_Q=np.array([0.725471,0.266683])) # the get_action function works\n",
    "env=RL_env(mydf) # stpe function works\n",
    "QLog,qlog=train_rat(env,rat,mydf.shape[0])\n",
    "qlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE is R-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
