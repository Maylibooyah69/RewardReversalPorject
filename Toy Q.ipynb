{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%config IPCompleter.greedy=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left 20%, Right 80%\n",
    "\n",
    "suppose the probability distribution of getting reward is constant.\n",
    "\n",
    "only two choise, left or right.\n",
    "\n",
    "there would be no states but the learning change would be marked by time\n",
    "\n",
    "the randomness of exploring instead of always choose the maximum expectation is modelled by sigmoid function, which makes such randomness constantly changing along time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: add in more policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants\n",
    "obs_size = 2 # get 0 or 1 as reward\n",
    "action_size = 2 #turn left or right\n",
    "alpha = 0.2 #learning rate\n",
    "beta = -6 # sharpness of sigmoid; this could be dynamic\n",
    "gamma = 0\n",
    "iteration = 80000\n",
    "every_it_print=1000\n",
    "\n",
    "# Env Policy\n",
    "left = 0.2 # probablity of reward in left\n",
    "right = 1 - left # probability of reward in right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function for nd; return the value of sigmoid with argument beta\n",
    "# q can be n dimensional, either list or int/float\n",
    "# beta should be between 0 and 1; determine the sharpness of sigmoid\n",
    "# we can also try different functions here\n",
    "def sigmoid(beta,q):\n",
    "    q=np.array(q)\n",
    "    val = 1/(1+np.exp(0-beta*q))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q table; 2x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Experiment\n",
    "# reward = (1,0) represent reward is on the left, (0,1) right\n",
    "# turn = (1,0) represent rat is on the left, (0,1) right\n",
    "\n",
    "l_rat = 0.5 # evaluation from rat on how likely the reward is on left\n",
    "r_rat = 0.5 # that for right\n",
    "reward_rat = 0 # reward got by rat by last action; = reward\n",
    "exp = 0 # action that maximize reward; 0 for left, 1 for right\n",
    "\n",
    "count = 0 # count for continuous success of choosing the advantageous action(here, the right)\n",
    "Q = np.zeros(2)\n",
    "\n",
    "#logs\n",
    "rewards=[]\n",
    "\n",
    "for i in range(iteration):\n",
    "            \n",
    "    # 0. get choice from rat \n",
    "    ratio = l_rat/(l_rat+r_rat)\n",
    "    if ratio > random.uniform(0,1):\n",
    "        turn = np.array([1,0])\n",
    "    else:\n",
    "        turn = np.array([0,1])\n",
    "\n",
    "    # 0. set reward \n",
    "    reward_site = random.uniform(0,1)\n",
    "    if reward_site < right and np.array_equal(turn,np.array([0,1])):\n",
    "    #if the rat goes right and the reward is on the right\n",
    "        reward = np.array((0,1))\n",
    "    elif reward_site >= right and np.array_equal(turn,np.array([1,0])):\n",
    "        reward = np.array((1,0))\n",
    "    else:\n",
    "        reward=np.array((0,0))\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # 2. update rat's information \n",
    "    Q = Q + alpha*turn*(reward + gamma*np.max(Q)-Q) # I think this is where it went wrong\n",
    "    # you should only update the action taken\n",
    "\n",
    "    l_rat = 1-(sigmoid(beta,Q[0])/(sigmoid(beta,Q[0])+sigmoid(beta,Q[1])))\n",
    "    r_rat=1-l_rat\n",
    "    # I changed this function\n",
    "    # 3. update count\n",
    "    if np.array_equal(turn,np.array([1,0])):\n",
    "        count = 0\n",
    "    elif np.array_equal(turn,np.array([0,1])):\n",
    "        count = count + 1\n",
    "        \n",
    "        \n",
    "    # Show ongoing results\n",
    "    if i%every_it_print==0:\n",
    "#         print('Q-table',Q)\n",
    "#         print(l_rat)\n",
    "        pass\n",
    "#         print(Q,reward)\n",
    "\n",
    "\n",
    "\n",
    "    # end experiment if rat made more than 10 times right turn, which has prob < 0.00098 by purely random choice\n",
    "#     if i == 280: #abs(r_rat-right < 0.01) # count > 10\n",
    "#         print('The rat might has learned somthing. Iteration number:')\n",
    "#         print(Q)\n",
    "\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.8498452012384"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)[1]/sum(rewards)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659378796930785"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(sigmoid(beta,0.8)/(sigmoid(beta,0.8)+sigmoid(beta,0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(random.uniform(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=[0.2,0.8]\n",
    "train_size=5000\n",
    "def get_reward(action,env): # 0 when reward is on the left\n",
    "    if np.random.uniform(0,1)<env[1]:\n",
    "        return int(action==1)\n",
    "    else:\n",
    "        return int(action==0)\n",
    "def get_action(Q):\n",
    "    return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.37357591e-01 7.98636218e+02]\n"
     ]
    }
   ],
   "source": [
    "# second implementation\n",
    "actions = np.array([0,1]) # 0 for left, 1 for right\n",
    "Q = np.random.uniform(low=0,high=1,size=(action_size))\n",
    "for i in range(train_size):\n",
    "    action=get_action(Q)\n",
    "    reward=get_reward(action,env)\n",
    "    Q[action]=(1-alpha)*Q[action]+alpha*(reward+gamma*np.max(Q))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a different implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toyQ_2choice:\n",
    "    '''a unchanging env with probability of giving one\n",
    "    of two rewards based on params to two different choices'''\n",
    "    obs_size=1\n",
    "    action_size=2\n",
    "    def __init__(self,left=0.2,right=0.8):\n",
    "        self.left=left\n",
    "        self.right=right\n",
    "    def step(self,action,state=None):\n",
    "        '''takes in the action param as a int of 0 or 1\n",
    "        where 0 is go left and 1 is to right, and returns\n",
    "        a return_site where 0 is left and 1 is right'''\n",
    "        reward_site=int(random.uniform(0,1)<self.right)\n",
    "        return int(reward_site==action)\n",
    "    \n",
    "class WSLS_rat: # to-do\n",
    "    pass\n",
    "\n",
    "class sig_rat:\n",
    "    def __init__(self,env,beta=-4,gamma=0):\n",
    "        '''Takes in the beta, gamma ,and the \n",
    "        environment of the rat'''\n",
    "        self.beta=beta\n",
    "        self.env=env\n",
    "        self.action_size=env.action_size\n",
    "        self.obs_size=env.obs_size\n",
    "        self.Q=np.zeros((self.obs_size,)+(self.action_size,))\n",
    "    def get_choice(self):\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((4,)+(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env=toyQ_2choice()\n",
    "rat=sig_rat(env)\n",
    "print(rat.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants\n",
    "obs_size = 2 # get 0 or 1 as reward\n",
    "action_size = 2 #turn left or right\n",
    "alpha = 0.2 #learning rate\n",
    "beta = -6 # sharpness of sigmoid; this could be dynamic\n",
    "gamma = 0\n",
    "iteration = 80000\n",
    "every_it_print=1000\n",
    "\n",
    "# Env Policy\n",
    "left = 0.2 # probablity of reward in left\n",
    "right = 1 - left # probability of reward in right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
